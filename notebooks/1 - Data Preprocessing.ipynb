{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359ba351",
   "metadata": {},
   "source": [
    "# 1 - Data Preprocessing\n",
    "\n",
    "This notebook takes in plain text files from Paris Review articles, applies preprocessing and exports them in a Bag-of-Words vector reprensetation for subsequent topic modelling in Notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b222a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec5973",
   "metadata": {},
   "source": [
    "Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a02da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data directory\n",
    "dir_data = Path(\"../data\")\n",
    "dir_raw = dir_data / \"raw\"\n",
    "# additional input files\n",
    "stoplist_file = \"stopwords.txt\"\n",
    "lem_file = \"lemmatization-en.txt\"\n",
    "# output paths\n",
    "dir_out = dir_data / \"proc\"\n",
    "out_prefix = \"paris\"\n",
    "\n",
    "# minimum length for a term (word)\n",
    "min_term_length = 2\n",
    "# minimum number of documents for a term to appear\n",
    "min_df = 10\n",
    "# minimum document length (in characters)\n",
    "min_doc_length = 80\n",
    "# maximum ngram range (default is 1, i.e. unigrams only)\n",
    "max_ngram = 1\n",
    "# weighting and normalization settings\n",
    "apply_tfidf = True\n",
    "apply_norm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dca65",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc4567",
   "metadata": {},
   "source": [
    "Find all relevant text files, where each file corresponds to a Paris Review article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75f6ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404 text files to preprocess\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for f in dir_raw.glob('**/*.txt'):\n",
    "    file_paths.append(f)\n",
    "file_paths.sort()\n",
    "print(\"Found %d text files to preprocess\" % len(file_paths))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4ec34",
   "metadata": {},
   "source": [
    "Read the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec50694",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = re.compile('https?[:;]?/?/?\\S*')\n",
    "\n",
    "def read_text(in_path):\n",
    "    body = \"\"\n",
    "    with open(in_path, 'r', encoding=\"utf8\", errors='ignore') as fin:\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            # Remove URIs at this point\n",
    "            normalized_line = re.sub(url_pattern, '', line.strip())\n",
    "            if len(normalized_line) > 1:\n",
    "                body += normalized_line\n",
    "                body += \"\\n\"\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9687c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 404 documents. Skipped 0 documents with length < 80\n"
     ]
    }
   ],
   "source": [
    "documents, document_ids = [], []\n",
    "num_short_documents = 0\n",
    "for in_path in file_paths:\n",
    "    # create the document ID\n",
    "    dirname = in_path.parent.name\n",
    "    doc_id = str(in_path)\n",
    "    # proess the body text\n",
    "    body = read_text(in_path)\n",
    "    if len(body) < min_doc_length:\n",
    "        num_short_documents += 1\n",
    "        continue    \n",
    "    documents.append(body)\n",
    "    document_ids.append(doc_id)\n",
    "print(\"Kept %d documents. Skipped %d documents with length < %d\" % \n",
    "    (len(documents), num_short_documents, min_doc_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f170e",
   "metadata": {},
   "source": [
    "Load stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c136f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 610 stopwords from stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open(stoplist_file) as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        l = l.strip().lower()\n",
    "        if len(l) > 0:\n",
    "            stopwords.append(l)\n",
    "print(\"Using %d stopwords from %s\" % (len(stopwords), stoplist_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391af338",
   "metadata": {},
   "source": [
    "Load lemmatization dictionary and apply it to our stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b060eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictLemmatizer:\n",
    "    def __init__(self, in_path):\n",
    "        self.term_map = {}\n",
    "        with open(in_path, 'r', encoding=\"utf8\", errors='ignore') as fin:\n",
    "            while True:\n",
    "                line = fin.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                parts = line.strip().lower().split(\"\\t\")\n",
    "                term = parts[1].strip()\n",
    "                stem = parts[0].strip()\n",
    "                if len(term) >= min_term_length and len(stem) >= min_term_length:\n",
    "                    self.term_map[term] = stem\n",
    "\n",
    "    def apply(self, s):\n",
    "        if not s in self.term_map:\n",
    "            return s\n",
    "        return self.term_map[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297c7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lemmatization dictionary from lemmatization-en.txt ...\n",
      "Using 498 stopwords after lemmatization\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading lemmatization dictionary from %s ...\" % lem_file)\n",
    "lemmatizer = DictLemmatizer(lem_file)\n",
    "extra_stopwords = set()\n",
    "for stopword in stopwords:\n",
    "    extra_stopwords.add(lemmatizer.apply(stopword))\n",
    "stopwords = list(extra_stopwords)\n",
    "print(\"Using %d stopwords after lemmatization\" % len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761662d",
   "metadata": {},
   "source": [
    "## Bag-of-Words Preprocessing\n",
    "\n",
    "Define our word tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d526695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"\\b\\w\\w+\\b\", re.U)\n",
    "\n",
    "def custom_tokenizer(s):\n",
    "    return [x.lower() for x in token_pattern.findall(s) if len(x) >= min_term_length ]\n",
    "\n",
    "def unigram_tokenizer(s):\n",
    "    tokens = custom_tokenizer(s.lower())\n",
    "    if lemmatizer is None:\n",
    "        return \n",
    "    lem_tokens = []\n",
    "    for token in tokens:\n",
    "        ltoken = lemmatizer.apply(token)\n",
    "        if len(ltoken) >= min_term_length:\n",
    "            lem_tokens.append(ltoken)\n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbcb2c",
   "metadata": {},
   "source": [
    "Convert the documents to a vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ce696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data (498 stopwords, tfidf=True, normalize=True, min_df=10, max_ngram=1) ...\n",
      "Built document-term matrix: 404 documents, 7061 terms\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing data (%d stopwords, tfidf=%s, normalize=%s, min_df=%d, max_ngram=%d) ...\" % \n",
    "    (len(stopwords), apply_tfidf, apply_norm, min_df, max_ngram))\n",
    "\n",
    "if apply_norm:\n",
    "    norm_function = \"l2\"\n",
    "else:\n",
    "    norm_function = None\n",
    "\n",
    "# build the Vector Space Model, apply TF-IDF and normalize lines to unit length all in one call\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, lowercase=True, strip_accents=\"unicode\", token_pattern=None,\n",
    "    tokenizer=unigram_tokenizer, use_idf=apply_tfidf, norm=norm_function, \n",
    "    min_df=min_df, ngram_range=(1, max_ngram))\n",
    "X = tfidf.fit_transform(documents)\n",
    "terms = []\n",
    "# create the vocabulary map\n",
    "v = tfidf.vocabulary_\n",
    "for i in range(len(v)):\n",
    "    terms.append(\"\")\n",
    "for term in v.keys():\n",
    "    terms[v[term]] = term\n",
    "                        \n",
    "print(\"Built document-term matrix: %d documents, %d terms\" % (X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6b340",
   "metadata": {},
   "source": [
    "Save the preprocessed data in binary format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a67254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to ../data/proc/paris.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/proc/paris.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = \"%s.pkl\" % out_prefix\n",
    "out_path = dir_out  / fname\n",
    "dir_out.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving data to %s\" % out_path)\n",
    "joblib.dump((X,terms,document_ids), out_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
