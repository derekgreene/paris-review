{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55de0723",
   "metadata": {},
   "source": [
    "# 3 - Topic Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b96720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import joblib, itertools\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aab30",
   "metadata": {},
   "source": [
    "Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input directory\n",
    "dir_models = Path(\"../models\")\n",
    "# word embedding file to use for evaluation purposes\n",
    "embedding_path = Path(\"../embeddings\") / \"wikipedia2016-w2v-cbow-d100.bin\"\n",
    "\n",
    "# number of top terms to consider\n",
    "top = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6f3df",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f026b",
   "metadata": {},
   "source": [
    "Find all topic model descriptor files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for f in dir_models.glob('**/*ranks*.pkl'):\n",
    "    file_paths.append(f)\n",
    "file_paths.sort()\n",
    "print(\"Found %d text model files to load\" % len(file_paths))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e8afa",
   "metadata": {},
   "source": [
    "Load the word embedding model used during evaluation. This should be stored in the binary word2vec format used by Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading word embedding from %s ...\" % embedding_path)\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "vocab = set(embedding.index_to_key)\n",
    "print(\"Embedding has vocabulary of size %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afb6e1",
   "metadata": {},
   "source": [
    "## Topic Model Evaluation Metrics\n",
    "\n",
    "Implementation of an embedding-based topic distinctiveness score, where normalization is loosely based on the min-max cluster similarity measure proposed by Ding et al (2001). Note that for this measure better models will have lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScore:\n",
    "    def __init__(self, embedding):\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def evaluate_model(self, descriptors):\n",
    "        \"\"\" Calculate the overall model score based on the mean score across all unique pairs\n",
    "        of topics \"\"\"\n",
    "        topic_pair_scores = []\n",
    "        for descriptor1, descriptor2 in itertools.combinations(descriptors, 2):\n",
    "            sim = self.evaluate_similarity(descriptor1, descriptor2)\n",
    "            topic_pair_scores.append(sim)\n",
    "        return np.array(topic_pair_scores).mean()\n",
    "\n",
    "    def evaluate_similarity(self, descriptor1, descriptor2):\n",
    "        \"\"\" Calculate the normalized similarity score \"\"\"\n",
    "        numer = self.evaluate_raw_similarity(descriptor1, descriptor2)\n",
    "        denom = (self.evaluate_raw_similarity(descriptor1, descriptor1) * self.evaluate_raw_similarity(descriptor2, descriptor2))\n",
    "        if denom == 0:\n",
    "            return 0.0\n",
    "        return numer/denom\n",
    "\n",
    "    def evaluate_raw_similarity(self, descriptor1, descriptor2):\n",
    "        \"\"\" Calculate the raw (non-normalized) similarity score \"\"\"\n",
    "        pair_scores = []\n",
    "        for term1 in descriptor1:\n",
    "            for term2 in descriptor2:\n",
    "                if term1 in self.embedding and term2 in self.embedding:\n",
    "                    # threshold negative values\n",
    "                    s = max(self.embedding.similarity(term1, term2), 0)\n",
    "                    pair_scores.append(s)\n",
    "        if len(pair_scores) == 0:\n",
    "            return 0.0\n",
    "        return np.array(pair_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd136d5d",
   "metadata": {},
   "source": [
    "## Topic Model Evaluation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24d15c",
   "metadata": {},
   "source": [
    "Process each topic model results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b14798",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MinMaxScore(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Processing %d topic models ...\" % len(file_paths))\n",
    "scores = {}\n",
    "for in_path in file_paths:\n",
    "    k = int(in_path.parent.name.split(\"_k\")[1])\n",
    "    print(\"k=%02d: %s\" % (k,in_path))\n",
    "    term_rankings, _ = joblib.load(in_path)\n",
    "    # only take the top terms for the topics\n",
    "    truncated_rankings = []\n",
    "    for ranking in term_rankings:\n",
    "        truncated_rankings.append(ranking[0:min(len(ranking),top)])\n",
    "    # apply the evaluation metric\n",
    "    scores[k] = metric.evaluate_model(truncated_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedd5b8",
   "metadata": {},
   "source": [
    "Analyse the scores for the different models. Note a lower score is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.Series(scores, name=\"min-max\").to_frame()\n",
    "# list best models\n",
    "df_scores.sort_values(by=\"min-max\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plot of score vs number of topics\n",
    "ax = df_scores.plot(fontsize=13, color=\"darkorange\", figsize=(9, 5.5))\n",
    "kmin, kmax = min(df_scores.index), max(df_scores.index)\n",
    "ax.get_legend().remove()\n",
    "ax.set_xlabel(\"Number of Topics ($k$)\", fontsize=13)\n",
    "ax.set_ylabel(\"Min-Max Score\", fontsize=13)\n",
    "ax.set_xlim(kmin, kmax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5211df7",
   "metadata": {},
   "source": [
    "Export the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
